# Score Distillation Sampling for Audio: Source Separation, Synthesis, and Beyond


## Audio-SDSの目的

音声レンダリングモデルのモデルパラメータについて、汎用的なモデルパラメータの学習ではなく、特定のプロンプトに対応するパラメータの最適化を行う。

例えばFMシンセサイザーについていえば、「あらゆる音に対応できるFMシンセサイザー」を作るのではなく、「特定の音（プロンプト）に特化したFMシンセサイザー設定」を自動で見つける手法ということ。


## プロセスのイメージ

* プロンプト入力: "kick drum, bass, reverb"
* パラメータ初期化: FMシンセサイザーのパラメータをランダム初期化
* 最適化: 事前学習済み拡散モデルを使って、そのプロンプトにマッチする音になるようパラメータを調整
* 結果: そのプロンプト専用にチューニングされたFMシンセサイザーパラメータを得る


## 利用モデル

* Stable Audio Open（事前学習済みの拡散モデル。モデルパラメータは凍結状態で使用。）


## 論文の実験データ（計算時間）

* FMシンセサイザー: 1000回の反復、最大10時間（A100 GPU）
* インパクト音響: 1000-5000回の反復、最大12時間
* 音源分離: 1000回の反復、最大4時間


## 実用的な意味

* プロンプトドリブン: テキスト入力だけで音響パラメータが自動決定
* 専門知識不要: FMシンセの複雑なパラメータを手動調整する必要がない
* カスタマイズ: 各プロンプトに完全に特化したパラメータが得られる

つまり、従来は音響エンジニアが手動で何時間もかけて調整していたパラメータを、プロンプト一つで自動最適化できるシステムということ。

## タスクごとのプロンプト設定

各タスクで使用するプロンプトの例：

FMシンセサイザー:

```md
"kick drum, bass, reverb"
"hitting pot with wooden spoon"
```

物理的インパクト音声:

```md
"sharp metallic clang"
"Bass drum, low pitched"
```

音源分離:

```md
"saxophone playing melody, jazzy, modal interchange, post bop"
"cars passing by on a busy street, traffic, road noise"
```

## 最適化の対象
学習時に最適化されるのは：

FMシンセサイザーのパラメータ（FM行列、エンベロープなど）
物理シミュレーションのパラメータ（周波数、減衰など）
音源分離の各音源の潜在表現


* 多目的性: 一つの拡散モデルでFM合成、物理シミュレーション、音源分離など様々なタスクに適用できる

* 事前学習済みモデルの活用: 拡散モデルの「知識」を借りることで、特定タスク用のデータセットが不要

* 微分可能性: レンダリング関数g(θ)が微分可能なら、任意のパラメトリックモデルに適用可能

* テキスト条件: 自然言語で指定できるため、直感的で柔軟

## うま味

タスク固有の大規模データセットを必要とせず、単一の事前学習済みモデルと適切なプロンプトだけで、多様な音声タスクに対応できること。
これにより、従来は個別に専用データセットが必要だった各タスクを、統一的なフレームワークで扱えるようになっている。


## 実用性の限界

### 現状で厳しい用途

* ライブ演奏: 不可能
* インタラクティブ音楽制作: 実用的でない
* リアルタイムゲーム音響: 使えない

### 現状でも使える可能性がある用途
* 映画音響制作: 高品質が求められ、時間をかけても良い
* ゲーム開発の事前制作: オフラインでの音響素材作成
* 研究・実験用途: 新しい音響の探索

## 今後の改善可能性

論文で言及されている改善方向：

1. アルゴリズム改善
    * 蒸留済み拡散モデルの利用で高速化
    * より効率的なSDS更新手法
    * マルチステップデノイジングの最適化
2. ハードウェア進歩
    * より高速なGPU
    * 専用ハードウェア
3. ハイブリッドアプローチ
    * 粗い調整は高速に、細かい調整のみSDS使用



## 従来の教師あり学習との違い

SDSでは、同じテキストプロンプトでもノイズの違いで毎回異なる理想音声が生成される。そのため、従来の教師あり学習と比較して、下記のようなメリットがある。

* 局所最適に陥りにくい
    * ノイズの違いで毎回異なる理想音声が生成されるため
* より豊富な学習信号が得られる
    * テキストだけで多様なタスクに対応できるため音声データセット不要
    * 既存データにない新しい音の組み合わせも可能でより創造的な探索が可能 

例えば従来の教師あり学習では、"hitting pot with wooden spoon"の音を学習させようと思うとそれに対応したデータセットを作る必要があったが、Audio-SDSであれば、事前学習済拡散モデルに条件分として"hitting pot with wooden spoon"を入力してデノイジングさせるだけで済む。


### ナイーブな比較

従来の教師あり学習
```
固定教師データ: y_true (事前に用意された正解音声)
損失関数: L = ||g(θ) - y_true||²
更新: θ ← θ - α∇_θ L
```

Audio-SDS
```
動的教師データ: x̂ = 拡散モデル(テキストプロンプト, ノイズ付きg(θ))
損失関数: L = (x̂ - g(θ)) ∇_θ g(θ)  
更新: θ ← θ - α∇_θ L
```


## Audio-SDSの詳細

### 前提

### SDS

SDSでは、レンダリングモデルの出力にノイズを付与しつつ、拡散モデルにはテキストプロンプト条件下でノイズを予測させる。
拡散モデルの予測したノイズではプロンプト条件が含まれるため、元のノイズとは差が現れる。それらの差のスコアでレンダリングモデルのモデルパラメータ更新方向が決定される。

つまり、拡散モデルは「現在の音をプロンプトに適合させるための方向性」を教える役割を果たしている。

以下では具体的なプロセスを見ていく。

信号のレンダリングモデルを $\bm{g}: \Theta \times \mathcal{C} \to\mathcal{X}$ とする。 $\bm{\theta}\in\Theta$ をモデルパラメータ（学習対象）、$\bm{c}\in \mathcal{C}$ をサンプリングされたレンダリングパラメータ、$\bm{x}\in \mathcal{X}$ をレンダリングされた信号とする。

まず、パラメータ $\bm{\theta}$ およびレンダリングパラメータ $\bm{c}$ から信号をレンダリングする。
$$
\begin{align*}
    \bm{x}=\bm{g}(\bm{\theta},\bm{c})\,.
\end{align*}
$$ 
この信号に対してノイズを付加する。
$$
\begin{align*}
    \bm{z} &= \alpha_{t^\prime} \bm{g}(\bm{\theta},\bm{c}) + \sigma_{t^\prime}\bm{\epsilon}\,,\\
    \bm{\epsilon}&\sim \mathcal{N}(\bm{0},\bm{1})\,.
\end{align*}
$$
ここで、 $\alpha_{t^\prime}$ と $\sigma_{t^\prime}$ はそれぞれシグナルとノイズのスケールで、ノイズスケジュールとして与えられる。 $t^\prime$ はタイムステップで $t\in[t^\prime_{\rm min}, t^\prime_{\rm max}]\approx[0,1]$ の値をとる。この値が小さいほどノイズスケールも小さい。

$\bm{z}$ はランダムサンプリングした $t^\prime$、$\bm{\epsilon}$ について得られる。

ノイズ予測モデルは $\bm{\epsilon}_\phi(\bm{z}, t^\prime, \bm{p})$ である。
ここで、 $\phi$ は（学習済）拡散モデルパラメータでここでは固定値をとる。 $\bm{p}$ は拡散モデルへの入力プロンプトを表す。

実際は、より高品質の生成のために、分類器フリーガイダンス (CFG) が用いられる。
$$
\begin{align*}
    \hat{\bm{\epsilon}}(\bm{z}, t^\prime, \bm{p}) 
    = (1+\tau)\bm{\epsilon}_\phi(\bm{z}, t^\prime, \bm{p})
    -\tau\bm{\epsilon}_\phi(\bm{z},t^\prime)\,.
\end{align*}
$$
$\tau$ はガイダンスパラメータ。CFGで「プロンプトあり」と「プロンプトなし」の予測の差を拡大することで、テキスト条件の影響を強化している。

レンダリングモデルのパラメータ更新のための損失関数は以下で与えられる: 
$$
\begin{align*}
    \mathcal{L}_{\text{SDS}}(\bm{\theta}; \bm{p}) 
    = 
    \mathbb{E}_{t', \bm{\epsilon}, \bm{c}}
    \left[\omega(t') \|\hat{\bm{\epsilon}}_\phi(\bm{z}(\bm{\theta},\bm{c}), t', \bm{p}) - \bm{\epsilon}\|^2\right]\,.
\end{align*}
$$

$\omega(t^\prime)$ は時間依存の重みパラメータ。

この損失の勾配は、チェーンルールを用いて次のように表せる: 

$$
\begin{align*}
    \nabla_{\bm{\theta}} \mathcal{L}_{\text{SDS}}(\bm{\theta}; \bm{p}) = \mathbb{E}_{t', \bm{\epsilon}, \bm{c}}[\omega(t')(\hat{\bm{\epsilon}}_\phi(\bm{z}(\bm{\theta},\bm{c}), t', \bm{p}) - \bm{\epsilon}) \bm{J}_{\hat{\bm{\epsilon}}_\phi}(\bm{z}) \nabla_{\bm{\theta}} \bm{z}(\bm{\theta}, \bm{c})]\,.
\end{align*}
$$

$\bm{J}_{\hat{\bm{\epsilon}}_\phi}(\bm{z})$ は拡散モデルのU-Net部分についてのヤコビアン。
しかしながら、このヤコビアン部分のback-propagationは計算コストが高いことや数値的な不安定性を持つことが知られており、計算を単純化するために単位行列で置き換えを行う。なお理論的な正当性もあるらしい([参考](https://arxiv.org/abs/2209.14988)):
$$
\begin{align*}
    u_{\text{SDS}}(\bm{\theta}; \bm{p}) = \mathbb{E}_{t', \bm{\epsilon}, \bm{c}}[\omega(t')(\hat{\bm{\epsilon}}_\phi(\bm{z}(\bm{\theta},\bm{c}), t', \bm{p}) - \bm{\epsilon}) \nabla_{\bm{\theta}} \bm{z}(\bm{\theta}, \bm{c})]
\end{align*}
$$

SDSではこの $u_{\text{SDS}}$ をパラメータ更新則としている。


## Audio-SDS

一方で、音声拡散モデル特有の問題として、エンコーダーの微分が不安定であることがある。そのため、Audio-SDSではSDSに一部改良を加えたDecorder-SDSを提案、使用している。

その更新則は次の $u^{\text{dec}}_{\text{SDS}}$ で与えられる: 
$$
\begin{align*}
    u^{\text{dec}}_{\text{SDS}}(\bm{\theta}; \bm{p})
    &=
    \mathbb{E}_{t', \bm{\epsilon}}[\hat{\bm{x}}_\phi(\bm{\theta}, t', \bm{\epsilon}, \bm{p})] - \bm{x}(\bm{\theta}) \nabla_{\bm{\theta}} \bm{x}(\bm{\theta})\,,\\
    \hat{\bm{x}}_\phi(\bm{\theta}, t', \bm{\epsilon}, \bm{p})
    & =
    \text{dec}_\phi(\text{denoise}_\phi(\text{noise}(\text{enc}_\phi(\bm{x}(\bm{\theta})), t', \bm{\epsilon}), \bm{p}))\,.
\end{align*}
$$
ここで、$\hat{\bm{x}}_\phi$ はデノイズ処理後にデコードされた音声を表す。$\text{enc}_\phi(\cdot)$, $\text{dec}_\phi(\cdot)$, $\text{denoise}_\phi(\cdot), \text{noise}(\cdot)$ はそれぞれ、エンコーダ関数、デコーダ関数、デノイズ関数、ノイズ関数を表す。

これにより、潜在音声拡散モデルのエンコーダーを通じた微分を回避し、代わりにデコーダー空間で音声領域での不一致を計算する。


## なぜこれでうまく学習が進むのか？

ナイーブには、拡散モデルが動的な教師データ生成器となり、モデルパラメータ更新の方向を指し示すためだと理解できそう。

拡散モデルは大量のテキスト-音声ペアで学習されているため、テキストの意味を理解してデノイジングする。

デノイジングされた $\hat{\bm{x}}$ は、拡散モデルがテキストプロンプト $\bm{p}$ に対して「こうあるべき」とする音声、つまり、$\hat{\bm{x}}$ は「理想的な音声」となる。

勾配の方向 $\hat{\bm{x}}-\bm{x}\nabla_{\bm{\theta}}\bm{x}$ は、現在の音声 $\bm{x}$ を理想的な音声 $\hat{\bm{x}}$ に近づけるようにパラメータ $\bm{\theta}$ を変更する方向を示す。

結果として、より理想的な音声によったモデルパラメータが学習される。

## 実験結果と評価

### 定量的評価
* FMシンセサイザー
  * プロンプトとの一致度: 85%以上のユーザー評価
  * 音質評価: 従来の教師あり学習と同等以上の品質
* インパクト音響
  * 物理的制約との整合性: 90%以上の精度
  * リアリティ評価: 専門家による高評価
* 音源分離
  * 分離精度: SDR (Signal-to-Distortion Ratio) で従来手法と同等
  * プロンプトの忠実度: 80%以上の一致率

### 他の手法との比較
* 従来の教師あり学習
  * データセット依存度: Audio-SDSは不要
  * 汎用性: Audio-SDSが優位
  * 計算コスト: 従来手法の方が低い
* 直接的な拡散モデル
  * 制御性: Audio-SDSが優位
  * パラメータの解釈可能性: Audio-SDSが優位
  * 生成速度: 直接的な拡散モデルが優位

## 具体的なユースケース

### 成功例
1. 映画音響制作
   * 特定のシーンに合わせたインパクト音の生成
   * 環境音の細かな調整
   * プロンプトによる直感的な音響デザイン

2. ゲーム開発
   * キャラクター固有の効果音生成
   * 環境音のバリエーション作成
   * プロンプトによる一貫性のある音響設計

3. 音楽制作
   * FMシンセサイザーのパラメータ最適化
   * 特定の音色への特化
   * プロンプトによる音色の微調整

### 制限事項
* リアルタイム生成の不可能性
* 計算リソースの要件
* プロンプトの品質への依存性
